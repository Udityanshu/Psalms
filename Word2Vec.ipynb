{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referance/Copied from for my personal referance \n",
    "\n",
    "https://www.geeksforgeeks.org/implement-your-own-word2vecskip-gram-model-in-python/\n",
    "\n",
    "https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import string \n",
    "from nltk.corpus import stopwords  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x)) \n",
    "    return e_x / e_x.sum() \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec(object): \n",
    "    def __init__(self): \n",
    "        self.N = 10\n",
    "        self.X_train = [] \n",
    "        self.y_train = [] \n",
    "        self.window_size = 2\n",
    "        self.alpha = 0.001\n",
    "        self.words = [] \n",
    "        self.word_index = {} \n",
    "   \n",
    "    def initialize(self,V,data): \n",
    "        self.V = V \n",
    "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N)) \n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V)) \n",
    "           \n",
    "        self.words = data \n",
    "        for i in range(len(data)): \n",
    "            self.word_index[data[i]] = i \n",
    "   \n",
    "       \n",
    "    def feed_forward(self,X): \n",
    "        self.h = np.dot(self.W.T,X).reshape(self.N,1) \n",
    "        self.u = np.dot(self.W1.T,self.h) \n",
    "        #print(self.u) \n",
    "        self.y = softmax(self.u)   \n",
    "        #print(self.y)\n",
    "        return self.y \n",
    "           \n",
    "    def backpropagate(self,x,t): \n",
    "        e = self.y - np.asarray(t).reshape(self.V,1) \n",
    "        # e.shape is V x 1 \n",
    "        dLdW1 = np.dot(self.h,e.T) \n",
    "        X = np.array(x).reshape(self.V,1) \n",
    "        dLdW = np.dot(X, np.dot(self.W1,e).T) \n",
    "        self.W1 = self.W1 - self.alpha*dLdW1 \n",
    "        self.W = self.W - self.alpha*dLdW \n",
    "           \n",
    "    def train(self,epochs): \n",
    "        for x in range(1,epochs):         \n",
    "            self.loss = 0\n",
    "            for j in range(len(self.X_train)): \n",
    "                self.feed_forward(self.X_train[j]) \n",
    "                self.backpropagate(self.X_train[j],self.y_train[j]) \n",
    "                C = 0\n",
    "                for m in range(self.V): \n",
    "                    if(self.y_train[j][m]): \n",
    "                        self.loss += -1*self.u[m][0] \n",
    "                        #print(m,  \"---m\")\n",
    "                        C += 1\n",
    "                self.loss += C*np.log(np.sum(np.exp(self.u))) \n",
    "            print(\"epoch \",x, \" loss = \",self.loss) \n",
    "            self.alpha *= 1/( (1+self.alpha*x) ) \n",
    "        print(self.W)\n",
    "              \n",
    "    def predict(self,word,number_of_predictions): \n",
    "        if word in self.words: \n",
    "            index = self.word_index[word] \n",
    "            X = [0 for i in range(self.V)] \n",
    "            X[index] = 1\n",
    "            prediction = self.feed_forward(X) \n",
    "            output = {} \n",
    "            for i in range(self.V): \n",
    "                output[prediction[i][0]] = i \n",
    "               \n",
    "            top_context_words = [] \n",
    "            for k in sorted(output,reverse=True): \n",
    "                top_context_words.append(self.words[output[k]]) \n",
    "                if(len(top_context_words)>=number_of_predictions): \n",
    "                    break\n",
    "       \n",
    "            return top_context_words \n",
    "        else: \n",
    "            print(\"Word not found in dicitonary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus): \n",
    "    stop_words = set(stopwords.words('english'))     \n",
    "    training_data = [] \n",
    "    sentences = corpus.split(\".\") \n",
    "    for i in range(len(sentences)): \n",
    "        sentences[i] = sentences[i].strip() \n",
    "        sentence = sentences[i].split() \n",
    "        x = [word.strip(string.punctuation) for word in sentence \n",
    "                                     if word not in stop_words] \n",
    "        x = [word.lower() for word in x] \n",
    "        training_data.append(x) \n",
    "    return training_data \n",
    "       \n",
    "def prepare_data_for_training(sentences,w2v): \n",
    "    data = {} \n",
    "    for sentence in sentences: \n",
    "        for word in sentence: \n",
    "            if word not in data: \n",
    "                data[word] = 1\n",
    "            else: \n",
    "                data[word] += 1\n",
    "    \n",
    "    V = len(data) \n",
    "    data = sorted(list(data.keys())) \n",
    "\n",
    "    vocab = {} \n",
    "    for i in range(len(data)): \n",
    "        vocab[data[i]] = i \n",
    "    \n",
    "    for sentence in sentences: \n",
    "        for i in range(len(sentence)): \n",
    "            center_word = [0 for x in range(V)] \n",
    "            center_word[vocab[sentence[i]]] = 1\n",
    "            \n",
    "            context = [0 for x in range(V)] \n",
    "                          \n",
    "            for j in range(i-w2v.window_size,i+w2v.window_size): \n",
    "                \n",
    "                if i!=j and j>=0 and j<len(sentence): \n",
    "                    context[vocab[sentence[j]]] += 1\n",
    "            w2v.X_train.append(center_word) \n",
    "            w2v.y_train.append(context) \n",
    "            \n",
    "    w2v.initialize(V,data) \n",
    "    \n",
    "   \n",
    "    return w2v.X_train,w2v.y_train   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1  loss =  3843.339655409778\n",
      "epoch  2  loss =  3839.553637343515\n",
      "epoch  3  loss =  3835.785623493039\n",
      "epoch  4  loss =  3832.03895924129\n",
      "epoch  5  loss =  3828.3168978205545\n",
      "epoch  6  loss =  3824.622583320327\n",
      "epoch  7  loss =  3820.959035030192\n",
      "epoch  8  loss =  3817.3291333049765\n",
      "epoch  9  loss =  3813.7356071040363\n",
      "epoch  10  loss =  3810.181023318409\n",
      "epoch  11  loss =  3806.667777960095\n",
      "epoch  12  loss =  3803.1980892487686\n",
      "epoch  13  loss =  3799.7739925931\n",
      "epoch  14  loss =  3796.3973374291304\n",
      "epoch  15  loss =  3793.069785845838\n",
      "epoch  16  loss =  3789.79281290088\n",
      "epoch  17  loss =  3786.5677085062503\n",
      "epoch  18  loss =  3783.3955807457655\n",
      "epoch  19  loss =  3780.2773604730564\n",
      "epoch  20  loss =  3777.213807030447\n",
      "epoch  21  loss =  3774.2055149254593\n",
      "epoch  22  loss =  3771.25292130164\n",
      "epoch  23  loss =  3768.3563140444426\n",
      "epoch  24  loss =  3765.515840369649\n",
      "epoch  25  loss =  3762.731515750748\n",
      "epoch  26  loss =  3760.0032330530526\n",
      "epoch  27  loss =  3757.3307717542384\n",
      "epoch  28  loss =  3754.7138071443082\n",
      "epoch  29  loss =  3752.1519194110842\n",
      "epoch  30  loss =  3749.644602530811\n",
      "epoch  31  loss =  3747.19127289635\n",
      "epoch  32  loss =  3744.7912776276644\n",
      "epoch  33  loss =  3742.4439025208676\n",
      "epoch  34  loss =  3740.148379602686\n",
      "epoch  35  loss =  3737.903894266695\n",
      "epoch  36  loss =  3735.7095919762382\n",
      "epoch  37  loss =  3733.564584526328\n",
      "epoch  38  loss =  3731.4679558633406\n",
      "epoch  39  loss =  3729.4187674665195\n",
      "epoch  40  loss =  3727.4160633002043\n",
      "epoch  41  loss =  3725.4588743489066\n",
      "epoch  42  loss =  3723.546222750662\n",
      "epoch  43  loss =  3721.6771255459466\n",
      "epoch  44  loss =  3719.8505980614086\n",
      "epoch  45  loss =  3718.0656569484036\n",
      "epoch  46  loss =  3716.3213228971945\n",
      "epoch  47  loss =  3714.6166230478516\n",
      "epoch  48  loss =  3712.9505931188\n",
      "epoch  49  loss =  3711.3222792737047\n",
      "epoch  50  loss =  3709.730739746803\n",
      "epoch  51  loss =  3708.17504624615\n",
      "epoch  52  loss =  3706.6542851533895\n",
      "epoch  53  loss =  3705.167558537861\n",
      "epoch  54  loss =  3703.713985001813\n",
      "epoch  55  loss =  3702.292700372483\n",
      "epoch  56  loss =  3700.902858255966\n",
      "epoch  57  loss =  3699.543630466616\n",
      "epoch  58  loss =  3698.21420734482\n",
      "epoch  59  loss =  3696.9137979751185\n",
      "epoch  60  loss =  3695.6416303154515\n",
      "epoch  61  loss =  3694.396951247792\n",
      "epoch  62  loss =  3693.179026559323\n",
      "epoch  63  loss =  3691.987140862523\n",
      "epoch  64  loss =  3690.8205974620014\n",
      "epoch  65  loss =  3689.6787181749814\n",
      "epoch  66  loss =  3688.5608431117066\n",
      "epoch  67  loss =  3687.466330421688\n",
      "epoch  68  loss =  3686.3945560107777\n",
      "epoch  69  loss =  3685.344913233815\n",
      "epoch  70  loss =  3684.316812566985\n",
      "epoch  71  loss =  3683.3096812636218\n",
      "epoch  72  loss =  3682.3229629968614\n",
      "epoch  73  loss =  3681.356117491972\n",
      "epoch  74  loss =  3680.4086201511386\n",
      "epoch  75  loss =  3679.4799616729483\n",
      "epoch  76  loss =  3678.569647668669\n",
      "epoch  77  loss =  3677.6771982771133\n",
      "epoch  78  loss =  3676.8021477796683\n",
      "epoch  79  loss =  3675.944044216885\n",
      "epoch  80  loss =  3675.102449007802\n",
      "epoch  81  loss =  3674.2769365730387\n",
      "epoch  82  loss =  3673.4670939625817\n",
      "epoch  83  loss =  3672.6725204888903\n",
      "epoch  84  loss =  3671.8928273661713\n",
      "epoch  85  loss =  3671.127637356082\n",
      "epoch  86  loss =  3670.37658442055\n",
      "epoch  87  loss =  3669.639313381807\n",
      "epoch  88  loss =  3668.9154795902073\n",
      "epoch  89  loss =  3668.20474859971\n",
      "epoch  90  loss =  3667.5067958514787\n",
      "epoch  91  loss =  3666.8213063654703\n",
      "epoch  92  loss =  3666.147974440223\n",
      "epoch  93  loss =  3665.4865033607607\n",
      "epoch  94  loss =  3664.8366051147036\n",
      "epoch  95  loss =  3664.198000116365\n",
      "epoch  96  loss =  3663.5704169390337\n",
      "epoch  97  loss =  3662.9535920550193\n",
      "epoch  98  loss =  3662.3472695837045\n",
      "epoch  99  loss =  3661.7512010471155\n",
      "[[ 0.46868124 -0.43209384  0.45629973 ...  0.13559768 -0.50222287\n",
      "   0.02864333]\n",
      " [-0.56266833  0.66050174 -0.24984268 ...  0.68693386  0.01964333\n",
      "   0.26057188]\n",
      " [ 0.0134923   0.7591396   0.32469331 ... -0.08474001  0.65546867\n",
      "  -0.53567521]\n",
      " ...\n",
      " [-0.7172485   0.89348688  0.44627609 ... -0.2294541   0.00285808\n",
      "  -0.29419874]\n",
      " [ 0.10949195 -0.35090509 -0.29254382 ... -0.21905594 -0.76978734\n",
      "   0.44013451]\n",
      " [-0.2613491   0.82231505  0.50842862 ...  0.5728657   0.76500728\n",
      "  -0.82779166]]\n",
      "['way', 'tomar', 'secretary']\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\" \n",
    "corpus += \"Prime Minister Narendra Modi was accorded a warm welcome by members of the Indian community in Houston on September 21 as he arrived for the mega Howdy Modi event in which he will be joined by U.S. President Donald Trump and address over 50,000 Indian-Americans.On the way to Houston, Mr. Modi’s flight had a two-hour technical halt in Frankfurt early on September 21 where he was received by India’s Ambassador to Germany Mukta Tomar and Consul General Pratibha Parkar.The Howdy Modi event on September 22 at the sprawling NRG Football Stadium in Houston is the largest gathering ever for an elected foreign leader visiting the U.S., other than the Pope.Mr. Trump’s presence at the Houston event marks a new milestone, Mr. Modi had said in his departure statement ahead of his visit. This would be the first time a U.S. President is attending an Indian-American community event with Modi.In Houston, Mr. Modi will also interact with CEOs of leading American energy companies with an aim to enhance India-US energy partnership.From Houston, Mr. Modi will fly to New York where he will address the annual high-level UN General Assembly session in New York on September 27 and will have a packed agenda of bilateral and multilateral engagements.Mr. Modi will met Mr. Trump in New York on September 24, the fourth meeting between the two leaders in as many months. The meeting in New York is expected to set the tone of bilateral relationship between the two countries over the next few years.The two leaders are likely to discuss a range of bilateral, regional and global issues, including efforts to address the growing bilateral trade disputes, potential defence and energy deals and peace process in Afghanistan.In his departure statement, Mr. Modi referred to the Indo-U,S. relations and said working together, the two nations can contribute to building a more peaceful, stable, secure, sustainable and prosperous world.Mr. Modi also said that at the High Level Segment of the 74th Session of the United Nations General Assembly, he would reiterate New Delhi’s commitment to reformed multilateralism, which is responsive, effective and inclusive, and in which India plays her due role.Foreign Secretary Vijay Gokhale in New Delhi on September 19 said the focus of Mr. Modi’s week-long visit to the U.S. to attend the UNGA will not be on terrorism, but on highlighting India’s achievements and its global role.He also asserted that abrogation of provisions of Article 370 from Jammu and Kashmir was an internal issue and off the agenda at the UN.\"\n",
    "epochs = 100\n",
    "  \n",
    "training_data = preprocessing(corpus) \n",
    "w2v = word2vec() \n",
    "  \n",
    "prepare_data_for_training(training_data,w2v) \n",
    "w2v.train(epochs)  \n",
    "  \n",
    "print(w2v.predict(\"welcome\",3))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
